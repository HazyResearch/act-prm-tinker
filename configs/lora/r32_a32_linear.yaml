r: 32
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - "self_attn.q_proj"
  - "self_attn.k_proj"
  - "self_attn.v_proj"
  - "self_attn.o_proj"
  - "mlp.down_proj"
  - "mlp.gate_proj"
  - "mlp.up_proj"