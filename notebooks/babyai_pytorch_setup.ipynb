{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BabyAI PyTorch Training Setup (Colab)\n\nThis notebook contains all the setup steps needed to run Act-PRM training on BabyAI using PyTorch in Google Colab.\n\n## Cell Info:\n1. **Cell 1**: Clone act-prm-tinker repository (`pytorch` branch + BabyAI files from `madison/babyai`)\n2. **Cell 2**: Install PyTorch ecosystem (may require runtime restart)\n3. **Cell 3**: Install act-prm package and dependencies\n4. **Cell 4**: Install BabyAI dependencies (gym, babyai-text, babyai, gym-minigrid)\n5. **Cell 5**: Authenticate with Hugging Face and W&B\n6. **Cell 6**: Fix model config (remove FlashAttention2)\n7. **Cell 7**: Post-restart setup: BabyAI sys.path (run after every kernel restart)\n8. **Cell 8**: Run training\n\n## Important Notes:\n- The PyTorch training infrastructure lives on the `pytorch` branch. The BabyAI environment and configs live on `madison/babyai`. Cell 1 checks out `pytorch` and cherry-picks the BabyAI files.\n- BabyAI depends on the **old** `gym` (not `gymnasium`) and is **incompatible with NumPy >= 2.0** out of the box. A compatibility shim in `env.py` patches `np.bool8`; see `src/act_prm/environments/babyai_text/README.md` for details.\n- After installing PyTorch (Cell 2), you may need to **restart the runtime** and then continue from Cell 3."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone act-prm-tinker Repository\n# Checks out 'pytorch' branch, then cherry-picks BabyAI files from 'madison/babyai'\nimport os\n\n# Get token from Colab secrets (if you set it up) or prompt user\ngithub_token = os.environ.get(\"GITHUB_TOKEN\", \"\")\n\nif not github_token:\n    from getpass import getpass\n    github_token = getpass(\"Paste your GitHub Personal Access Token: \")\n\nrepo_path = '/content/act-prm-tinker'\nif os.path.exists(repo_path):\n    print(f\"✓ Repository already exists at {repo_path}\")\n    %cd {repo_path}\n    print(\"Pulling latest changes...\")\n    !git pull origin pytorch || echo \"Note: Could not pull pytorch branch\"\nelse:\n    print(f\"Cloning repository to {repo_path}...\")\n    !git clone https://{github_token}@github.com/HazyResearch/act-prm-tinker.git {repo_path}\n    %cd {repo_path}\n    !git checkout pytorch\n    print(\"✓ Checked out pytorch branch\")\n\n# Cherry-pick BabyAI environment files and configs from madison/babyai\nprint(\"\\nAdding BabyAI files from madison/babyai branch...\")\n!git fetch origin madison/babyai\n!git checkout origin/madison/babyai -- \\\n    src/act_prm/environments/babyai_text/ \\\n    configs/environments/babyai/ \\\n    configs/environments/act_prm/babyai.yaml\n\n# Patch __init__.py to register the babyai_text environment\ninit_path = 'src/act_prm/environments/__init__.py'\nwith open(init_path, 'r') as f:\n    content = f.read()\n\nif 'babyai_text' not in content:\n    babyai_block = '''\n    elif name == \"babyai_text\":\n        if is_async:\n            from .babyai_text import AsyncBabyAiTextEnv\n            return AsyncBabyAiTextEnv(**kwargs)\n        else:\n            from .babyai_text import BabyAiTextEnv\n            return BabyAiTextEnv(**kwargs)\n\n    raise NotImplementedError'''\n\n    content = content.replace(\n        '    raise NotImplementedError',\n        babyai_block,\n        1  # replace only the first occurrence\n    )\n    with open(init_path, 'w') as f:\n        f.write(content)\n    print(\"✓ Registered babyai_text in environment factory\")\nelse:\n    print(\"✓ babyai_text already registered in environment factory\")\n\nprint(f\"\\nCurrent directory: {os.getcwd()}\")\nprint(f\"Repository exists: {os.path.exists(repo_path)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Ecosystem (CUDA 12.1)\n",
    "# ⚠️ This cell may require a runtime restart after installation. After restart, continue from next cell\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_package_installed(package_name, version_check=None):\n",
    "    \"\"\"Check if a package is installed and optionally verify version\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'show', package_name],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            if version_check:\n",
    "                # Extract version from output\n",
    "                for line in result.stdout.split('\\n'):\n",
    "                    if line.startswith('Version:'):\n",
    "                        version = line.split(':', 1)[1].strip()\n",
    "                        print(f\"  Found {package_name} version: {version}\")\n",
    "                        return True\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"Checking current PyTorch installation...\")\n",
    "torch_installed = check_package_installed('torch')\n",
    "torchvision_installed = check_package_installed('torchvision')\n",
    "transformers_installed = check_package_installed('transformers')\n",
    "\n",
    "if torch_installed and torchvision_installed and transformers_installed:\n",
    "    print(\"✓ PyTorch ecosystem already installed\")\n",
    "    print(\"\\nVerifying versions...\")\n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        print(f\"  PyTorch: {torch.__version__}\")\n",
    "        print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "        print(f\"  Transformers: {transformers.__version__}\")\n",
    "        print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(\"\\n✓ PyTorch is ready. You can skip reinstalling if versions are compatible.\")\n",
    "        print(\"  If you need to reinstall, uncomment the lines below.\")\n",
    "    except ImportError as e:\n",
    "        print(f\"⚠ Import error: {e}\")\n",
    "        print(\"  Proceeding with installation...\")\n",
    "        torch_installed = False\n",
    "\n",
    "if not (torch_installed and torchvision_installed and transformers_installed):\n",
    "    print(\"\\nInstalling PyTorch ecosystem for CUDA 12.1...\")\n",
    "    print(\"⚠️  This may take a few minutes and may require a runtime restart.\")\n",
    "\n",
    "    # Uninstall existing versions\n",
    "    !pip uninstall -y torch torchvision transformers 2>/dev/null || true\n",
    "\n",
    "    # Install PyTorch and torchvision\n",
    "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 --no-cache-dir\n",
    "\n",
    "    # Install transformers and accelerate\n",
    "    !pip install transformers accelerate --no-cache-dir --upgrade\n",
    "\n",
    "    print(\"\\n✓ Installation complete!\")\n",
    "    print(\"⚠️  IMPORTANT: You may need to restart the runtime now.\")\n",
    "    print(\"   After restart, run Cell 4 (skip this cell).\")\n",
    "\n",
    "    # Verify installation\n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import transformers\n",
    "        print(f\"\\n✓ Verification:\")\n",
    "        print(f\"  PyTorch: {torch.__version__}\")\n",
    "        print(f\"  Torchvision: {torchvision.__version__}\")\n",
    "        print(f\"  Transformers: {transformers.__version__}\")\n",
    "        print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"\\n⚠️  Import failed: {e}\")\n",
    "        print(\"   Please restart the runtime and continue from Cell 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ACT-PRM Package and Dependencies\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Ensure we're in the repo directory\n",
    "repo_path = '/content/act-prm-tinker'\n",
    "if not os.path.exists(repo_path):\n",
    "    print(f\"✗ Repository not found at {repo_path}\")\n",
    "    print(\"  Please run Cell 2 first to clone the repository.\")\n",
    "else:\n",
    "    %cd {repo_path}\n",
    "    print(f\"✓ Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Check if act-prm is already installed\n",
    "    try:\n",
    "        import act_prm\n",
    "        print(f\"✓ act-prm already installed at: {act_prm.__file__}\")\n",
    "    except ImportError:\n",
    "        print(\"Installing act-prm package...\")\n",
    "        !pip install tinker-cookbook\n",
    "        !pip install -e .\n",
    "        print(\"✓ act-prm installed successfully\")\n",
    "\n",
    "    # Verify installation\n",
    "    try:\n",
    "        import act_prm\n",
    "        from act_prm.environments import get_env\n",
    "        print(f\"✓ act-prm can be imported\")\n",
    "        print(f\"  Location: {act_prm.__file__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"✗ Import failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install BabyAI Dependencies\nimport os\nimport sys\n\n%cd /content\n\n# Clone BabyAI repository if needed\nif not os.path.exists('/content/Grounding_LLMs_with_online_RL'):\n    print(\"Cloning BabyAI repository...\")\n    !git clone https://github.com/flowersteam/Grounding_LLMs_with_online_RL.git\n    print(\"✓ Repository cloned successfully\")\nelse:\n    print(\"✓ BabyAI repository already exists\")\n\n# Install gym (old API, NOT gymnasium) and supporting libraries\nprint(\"\\nInstalling gym and BabyAI dependencies...\")\n!pip install \"gym>=0.21,<0.27\" blosc colorama termcolor matplotlib\n\n# Install babyai-text\nprint(\"\\nInstalling babyai-text...\")\n%cd /content/Grounding_LLMs_with_online_RL/babyai-text\n!pip install -e . --no-deps\n\n# Install babyai\nprint(\"\\nInstalling babyai...\")\n%cd babyai\n!pip install -e . --no-deps\n\n# Install gym-minigrid\nprint(\"\\nInstalling gym-minigrid...\")\n%cd ../gym-minigrid\n!pip install -e . --no-deps\n\n# Add to Python path (important for imports after runtime restart)\nprint(\"\\nAdding BabyAI paths to sys.path...\")\nbabyai_paths = [\n    '/content/Grounding_LLMs_with_online_RL/babyai-text',\n    '/content/Grounding_LLMs_with_online_RL/babyai-text/babyai',\n    '/content/Grounding_LLMs_with_online_RL/babyai-text/gym-minigrid',\n]\n\nfor path in babyai_paths:\n    if os.path.exists(path) and path not in sys.path:\n        sys.path.insert(0, path)\n        print(f\"  ✓ Added: {path}\")\n    elif path in sys.path:\n        print(f\"  ✓ Already in path: {path}\")\n    else:\n        print(f\"  ⚠ Path does not exist: {path}\")\n\n# Test imports\nprint(\"\\nTesting BabyAI imports...\")\ntry:\n    import babyai_text\n    import babyai\n    import gym_minigrid\n    print(\"✓ All BabyAI packages can be imported\")\n    print(f\"  babyai_text location: {babyai_text.__file__}\")\n    print(f\"  babyai location: {babyai.__file__}\")\n    print(f\"  gym_minigrid location: {gym_minigrid.__file__}\")\nexcept ImportError as e:\n    print(f\"✗ Import failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    print(\"\\n⚠️  If imports fail, make sure all paths are correct and packages are installed.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "import wandb\n",
    "\n",
    "# Hugging Face authentication\n",
    "print(\"Hugging Face Login:\")\n",
    "token = getpass(\"Paste your Hugging Face token: \")\n",
    "print(\"✓ Hugging Face authenticated\")\n",
    "\n",
    "# Weights & Biases authentication\n",
    "print(\"\\nWeights & Biases Login:\")\n",
    "wandb_key = getpass(\"Paste your W&B API key: \")\n",
    "wandb.login(key=wandb_key)\n",
    "print(\"✓ Weights & Biases authenticated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/act-prm-tinker\n",
    "\n",
    "# Remove the flash_attention_2 requirement\n",
    "!sed -i '/attn_implementation: \"flash_attention_2\"/d' configs/model/hf_llama3_1_8b_inst.yaml\n",
    "\n",
    "# Verify the change\n",
    "!cat configs/model/hf_llama3_1_8b_inst.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/act-prm-tinker\n",
    "\n",
    "# Ensure BabyAI paths are in sys.path (in case kernel was restarted)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "babyai_paths = [\n",
    "    '/content/Grounding_LLMs_with_online_RL/babyai-text',\n",
    "    '/content/Grounding_LLMs_with_online_RL/babyai-text/babyai',\n",
    "    '/content/Grounding_LLMs_with_online_RL/babyai-text/gym-minigrid',\n",
    "]\n",
    "\n",
    "for path in babyai_paths:\n",
    "    if os.path.exists(path) and path not in sys.path:\n",
    "        sys.path.insert(0, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%cd /content/act-prm-tinker\n\nimport os\nos.environ[\"PYTHONPATH\"] = \"src:\" + \\\n    \"/content/Grounding_LLMs_with_online_RL/babyai-text:\" + \\\n    \"/content/Grounding_LLMs_with_online_RL/babyai-text/babyai:\" + \\\n    \"/content/Grounding_LLMs_with_online_RL/babyai-text/gym-minigrid:\" + \\\n    os.environ.get(\"PYTHONPATH\", \"\")\n\n# Run training\n!python main_pytorch.py \\\n  --env_config act_prm/babyai \\\n  --base_env_config babyai/default \\\n  --eval_env_config babyai/eval \\\n  --generator_config aprm_qwen3_ap \\\n  --trainer_config aprm_for_sft100 \\\n  --replay_buffer_config default \\\n  --model_config hf_llama3_1_8b_inst \\\n  --lora_config r32_a32_qkvo \\\n  --log_path ./logs \\\n  --save_rollouts_every 10 \\\n  --seed 42 \\\n  --replicate 0 \\\n  --verbose"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}